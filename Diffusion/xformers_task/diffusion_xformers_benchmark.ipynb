{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-03-28T16:23:50.028718Z",
          "iopub.status.busy": "2025-03-28T16:23:50.028474Z",
          "iopub.status.idle": "2025-03-28T16:27:26.145910Z",
          "shell.execute_reply": "2025-03-28T16:27:26.144716Z",
          "shell.execute_reply.started": "2025-03-28T16:23:50.028695Z"
        },
        "trusted": true,
        "id": "C7DGJp72Rpax",
        "outputId": "6bbf4b76-236d-48a6-bd0f-675405c4e77d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DiT'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 102 (delta 57), reused 33 (delta 33), pack-reused 20 (from 1)\u001b[K\n",
            "Receiving objects: 100% (102/102), 6.36 MiB | 27.04 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "/kaggle/working/DiT\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\n",
            "pylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
            "pylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.6.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n",
            "Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torchvision-0.21.0\n"
          ]
        }
      ],
      "source": [
        "# Clone the DiT repository\n",
        "!git clone https://github.com/facebookresearch/DiT.git\n",
        "%cd DiT\n",
        "\n",
        "# Install xformers\n",
        "!pip install -q xformers\n",
        "!pip install --upgrade torchvision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSYfqB0uRpay"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhcBR4akRpaz"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficient Transformer Attention Implementation with XFormers\n",
        "\n",
        "This code implements an efficient attention mechanism using the XFormers library to optimize memory usage in transformer models. Here's what it does:\n",
        "\n",
        "## Imports and Dependencies\n",
        "- Imports PyTorch and neural network modules\n",
        "- Imports DiT model variants from a local `models` module\n",
        "- Attempts to import XFormers' memory-efficient attention operation (raising an error if not installed)\n",
        "\n",
        "## XFormersAttention Class\n",
        "The class implements a drop-in replacement for standard attention that:\n",
        "- Maintains the same API as regular attention modules\n",
        "- Takes hidden size, number of heads, and dropout rate parameters\n",
        "- Projects input to query, key, and value representations\n",
        "- Applies proper scaling to query vectors before attention computation\n",
        "- Uses XFormers' memory-efficient attention implementation\n",
        "- Projects attention outputs back to the original dimensionality\n",
        "\n",
        "## Key Optimizations\n",
        "- Uses memory-efficient attention for better GPU utilization\n",
        "- Includes critical scaling factor for stable training\n",
        "- Maintains dimensional transformations needed for compatibility with DiT models\n",
        "\n",
        "The comment highlights a previously missing scaling operation that's now properly applied to query vectors before the attention mechanism."
      ],
      "metadata": {
        "id": "TBWv1dtaR962"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-28T16:46:38.588859Z",
          "iopub.status.busy": "2025-03-28T16:46:38.588494Z",
          "iopub.status.idle": "2025-03-28T16:46:38.596572Z",
          "shell.execute_reply": "2025-03-28T16:46:38.595421Z",
          "shell.execute_reply.started": "2025-03-28T16:46:38.588834Z"
        },
        "trusted": true,
        "id": "-xJ1E5eyRpaz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "from models import DiT, DiT_B_4  , DiT_XL_4\n",
        "\n",
        "# Import xformers memory efficient attention\n",
        "try:\n",
        "    from xformers.ops import memory_efficient_attention\n",
        "except ImportError:\n",
        "    raise ImportError(\"xformers is not installed.\")\n",
        "\n",
        "# Define the XFormersAttention module that mimics the original Attention API\n",
        "class XFormersAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "        self.scale = self.head_dim ** -0.5  # Critical scaling factor\n",
        "        self.qkv = nn.Linear(hidden_size, hidden_size * 3, bias=True)\n",
        "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "\n",
        "        # Apply scaling BEFORE attention\n",
        "        q = q * self.scale  # ← This was missing\n",
        "\n",
        "        attn_output = memory_efficient_attention(q, k, v)\n",
        "        attn_output = attn_output.transpose(1, 2).reshape(B, N, C)\n",
        "        return self.out_proj(attn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell to replace the attention model with xformers attetnion block\n"
      ],
      "metadata": {
        "id": "PkxfIbL5SHSb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-28T16:29:41.769022Z",
          "iopub.status.busy": "2025-03-28T16:29:41.768459Z",
          "iopub.status.idle": "2025-03-28T16:29:41.773651Z",
          "shell.execute_reply": "2025-03-28T16:29:41.772684Z",
          "shell.execute_reply.started": "2025-03-28T16:29:41.768993Z"
        },
        "trusted": true,
        "id": "tyzmL9bVRpa0"
      },
      "outputs": [],
      "source": [
        "def replace_attention_with_xformers(model):\n",
        "    \"\"\"\n",
        "    Replaces the attention module in each DiTBlock of the model with the XFormersAttention version.\n",
        "    \"\"\"\n",
        "    for block in model.blocks:\n",
        "        hidden_size = block.norm1.normalized_shape[0]\n",
        "        num_heads = block.attn.num_heads  # Using the original number of heads\n",
        "        # Replace with our XFormersAttention (dropout is set to 0.0; adjust if needed)\n",
        "        block.attn = XFormersAttention(hidden_size, num_heads, dropout=0.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking Function to compare basline model and Xformers model"
      ],
      "metadata": {
        "id": "shaJAM-6SOCv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-28T16:39:22.980056Z",
          "iopub.status.busy": "2025-03-28T16:39:22.979488Z",
          "iopub.status.idle": "2025-03-28T16:39:22.987422Z",
          "shell.execute_reply": "2025-03-28T16:39:22.986426Z",
          "shell.execute_reply.started": "2025-03-28T16:39:22.980015Z"
        },
        "trusted": true,
        "id": "maZ-yvNpRpa0"
      },
      "outputs": [],
      "source": [
        "def compare_attention_speed(model, device='cuda', num_images=50, num_warmup=10, num_repeats=100):\n",
        "    model = model.to(device).eval()\n",
        "    x = torch.randn(num_images, model.in_channels, 32, 32, device=device)\n",
        "    t = torch.randint(0, 1000, (num_images,), device=device)\n",
        "    y = torch.randint(0, 1000, (num_images,), device=device)\n",
        "\n",
        "    # Warm-up\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "        for _ in range(num_warmup):\n",
        "            _ = model(x, t, y)\n",
        "\n",
        "    # Timed runs\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "        for _ in range(num_repeats):\n",
        "            _ = model(x, t, y)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    elapsed = (time.time() - start) / num_repeats\n",
        "    print(f\"Avg time per {num_images} images: {elapsed:.5f}s\")\n",
        "    return elapsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-28T16:46:44.038927Z",
          "iopub.status.busy": "2025-03-28T16:46:44.038540Z",
          "iopub.status.idle": "2025-03-28T16:48:04.221630Z",
          "shell.execute_reply": "2025-03-28T16:48:04.220687Z",
          "shell.execute_reply.started": "2025-03-28T16:46:44.038902Z"
        },
        "trusted": true,
        "id": "H5uvBcBKRpa1",
        "outputId": "7446300a-0086-4754-dbdf-5e372b590722"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-d1f7831d6d4c>:8: FutureWarning:\n",
            "\n",
            "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "\n",
            "<ipython-input-11-d1f7831d6d4c>:15: FutureWarning:\n",
            "\n",
            "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg time per 50 images: 0.25898s\n",
            "Avg time per 50 images: 0.06053s\n",
            "Baseline model timing:\n",
            "Avg time per 50 images: 0.24864s\n",
            "\n",
            "XFormers-based model timing:\n",
            "Avg time per 50 images: 0.05422s\n",
            "\n",
            "Speedup: 4.59x faster with xformers attention.\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "model_baseline = DiT_XL_4()\n",
        "# Create a deep copy for the xformers-modified model and replace its attention modules\n",
        "model_xformers=deep.copy(model_basline)\n",
        "replace_attention_with_xformers(model_xformers):\n",
        "\n",
        "time_baseline = compare_attention_speed(model_baseline, num_images=50)\n",
        "time_xformers = compare_attention_speed(model_xformers, num_images=50)\n",
        "\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Baseline model timing:\")\n",
        "time_baseline = compare_attention_speed(model_baseline, device=device, num_images=50)\n",
        "\n",
        "print(\"\\nXFormers-based model timing:\")\n",
        "time_xformers = compare_attention_speed(model_xformers, device=device, num_images=50)\n",
        "\n",
        "speedup = time_baseline / time_xformers if time_xformers > 0 else float('inf')\n",
        "print(f\"\\nSpeedup: {speedup:.2f}x faster with xformers attention.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-28T16:50:43.079434Z",
          "iopub.status.busy": "2025-03-28T16:50:43.079075Z",
          "iopub.status.idle": "2025-03-28T16:52:03.234222Z",
          "shell.execute_reply": "2025-03-28T16:52:03.233232Z",
          "shell.execute_reply.started": "2025-03-28T16:50:43.079407Z"
        },
        "trusted": true,
        "id": "NoWJHsvKRpa1",
        "outputId": "922ba12e-723f-4ebe-85aa-5f0ce9d53bc3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-d1f7831d6d4c>:8: FutureWarning:\n",
            "\n",
            "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "\n",
            "<ipython-input-11-d1f7831d6d4c>:15: FutureWarning:\n",
            "\n",
            "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg time per 50 images: 0.25127s\n",
            "Avg time per 50 images: 0.06060s\n",
            "Baseline model timing:\n",
            "Avg time per 50 images: 0.25291s\n",
            "\n",
            "XFormers-based model timing:\n",
            "Avg time per 50 images: 0.05463s\n",
            "\n",
            "Speedup: 4.63x faster with xformers attention.\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "model_baseline = DiT_XL_4()\n",
        "\n",
        "# Create a deep copy for the xformers-modified model and replace its attention modules\n",
        "time_baseline = compare_attention_speed(model_baseline, num_images=50)\n",
        "time_xformers = compare_attention_speed(model_xformers, num_images=50)\n",
        "\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Baseline model timing:\")\n",
        "time_baseline = compare_attention_speed(model_baseline, device=device, num_images=50)\n",
        "\n",
        "print(\"\\nXFormers-based model timing:\")\n",
        "time_xformers = compare_attention_speed(model_xformers, device=device, num_images=50)\n",
        "\n",
        "speedup = time_baseline / time_xformers if time_xformers > 0 else float('inf')\n",
        "print(f\"\\nSpeedup: {speedup:.2f}x faster with xformers attention.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-28T17:01:10.309615Z",
          "iopub.status.busy": "2025-03-28T17:01:10.309273Z",
          "iopub.status.idle": "2025-03-28T17:05:09.511796Z",
          "shell.execute_reply": "2025-03-28T17:05:09.510667Z",
          "shell.execute_reply.started": "2025-03-28T17:01:10.309575Z"
        },
        "trusted": true,
        "id": "6uPr9BeARpa2",
        "outputId": "fc2268db-df5f-4d09-eb17-48f20d44ff11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing batch size: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-d1f7831d6d4c>:8: FutureWarning:\n",
            "\n",
            "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "\n",
            "<ipython-input-11-d1f7831d6d4c>:15: FutureWarning:\n",
            "\n",
            "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg time per 10 images: 0.05103s\n",
            "Avg time per 10 images: 0.01535s\n",
            "\n",
            "Testing batch size: 50\n",
            "Avg time per 50 images: 0.26693s\n",
            "Avg time per 50 images: 0.05673s\n",
            "\n",
            "Testing batch size: 100\n",
            "Avg time per 100 images: 0.45293s\n",
            "Avg time per 100 images: 0.11553s\n",
            "\n",
            "Testing batch size: 200\n",
            "Avg time per 200 images: 0.99126s\n",
            "Avg time per 200 images: 0.22171s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1ff7a4ca-0b00-4525-a324-e096a3f7f37e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1ff7a4ca-0b00-4525-a324-e096a3f7f37e\")) {                    Plotly.newPlot(                        \"1ff7a4ca-0b00-4525-a324-e096a3f7f37e\",                        [{\"hovertemplate\":\"Model=Baseline\\u003cbr\\u003eBatch Size=%{x}\\u003cbr\\u003eTime (seconds)=%{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Baseline\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"Baseline\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[10,50,100,200],\"xaxis\":\"x\",\"y\":[0.051032874584198,0.2669324684143066,0.4529337191581726,0.9912617206573486],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Model=XFormers\\u003cbr\\u003eBatch Size=%{x}\\u003cbr\\u003eTime (seconds)=%{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"XFormers\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"XFormers\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[10,50,100,200],\"xaxis\":\"x\",\"y\":[0.015347213745117187,0.05673261404037475,0.11552794694900513,0.22170565128326417],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Number of Images\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Time (seconds)\"}},\"legend\":{\"title\":{\"text\":\"Model\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Inference Time vs Batch Size\"},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1ff7a4ca-0b00-4525-a324-e096a3f7f37e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Test across different batch sizes\n",
        "batch_sizes = [10, 50, 100, 200]\n",
        "times_baseline, times_xformers = [], []\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    print(f\"\\nTesting batch size: {bs}\")\n",
        "    times_baseline.append(compare_attention_speed(model_baseline, device=device, num_images=bs))\n",
        "    times_xformers.append(compare_attention_speed(model_xformers, device=device, num_images=bs))\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Batch Size\": np.tile(batch_sizes, 2),\n",
        "    \"Time (s)\": times_baseline + times_xformers,\n",
        "    \"Model\": [\"Baseline\"]*len(batch_sizes) + [\"XFormers\"]*len(batch_sizes)\n",
        "})\n",
        "\n",
        "# Interactive line plot\n",
        "fig = px.line(\n",
        "    df,\n",
        "    x=\"Batch Size\",\n",
        "    y=\"Time (s)\",\n",
        "    color=\"Model\",\n",
        "    title=\"Inference Time vs Batch Size\",\n",
        "    markers=True,\n",
        "    labels={\"Time (s)\": \"Time (seconds)\"},\n",
        "    hover_data={\"Time (s)\": \":.4f\"}\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    hovermode=\"x unified\",\n",
        "    xaxis_title=\"Number of Images\",\n",
        "    yaxis_title=\"Time (seconds)\",\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "8kRo32aVRpa3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 298806,
          "sourceId": 1217826,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}